{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1e890e",
   "metadata": {},
   "source": [
    "# Spotify Artist Data Fetcher\n",
    "\n",
    "**Purpose:** Incrementally fetch artist data for Spotify tracks over multiple days.\n",
    "\n",
    "**Usage:**\n",
    "1. Run this notebook once per day (after 24h rate limit reset)\n",
    "2. It will automatically fetch the next batch of ~1,500 tracks\n",
    "3. Progress is saved to `artist_info_extended.csv`\n",
    "4. Continue until you've fetched all 28K tracks!\n",
    "\n",
    "**Current Progress:** Check the status cell below to see how many tracks you've collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdeac78",
   "metadata": {},
   "source": [
    "## 1. Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing packages (runs only in Jupyter)\n",
    "%pip install -q spotipy python-dotenv\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d67225",
   "metadata": {},
   "source": [
    "## 2. Load Spotify API Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get credentials\n",
    "client_id = os.getenv('SPOTIFY_CLIENT_ID')\n",
    "client_secret = os.getenv('SPOTIFY_CLIENT_SECRET')\n",
    "\n",
    "# Initialize Spotify client\n",
    "auth_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "print(\"âœ“ Spotify API client initialized\")\n",
    "print(f\"âœ“ Client ID: {client_id[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370076f",
   "metadata": {},
   "source": [
    "## 3. Load Source Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93070e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main Spotify dataset\n",
    "df = pd.read_csv('spotify_songs.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SOURCE DATASET LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total tracks in source: {len(df):,}\")\n",
    "print(f\"Unique track IDs: {df['track_id'].nunique():,}\")\n",
    "\n",
    "# Get unique tracks only\n",
    "df_unique = df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n",
    "print(f\"\\nUnique tracks to potentially fetch: {len(df_unique):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f8c74",
   "metadata": {},
   "source": [
    "## 4. Check Current Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have existing progress\n",
    "PROGRESS_FILE = 'artist_info_extended.csv'\n",
    "\n",
    "if os.path.exists(PROGRESS_FILE):\n",
    "    df_existing = pd.read_csv(PROGRESS_FILE)\n",
    "    already_fetched = set(df_existing['track_id'].tolist())\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š CURRENT PROGRESS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"âœ“ Found existing progress file: {PROGRESS_FILE}\")\n",
    "    print(f\"âœ“ Already fetched: {len(already_fetched):,} tracks\")\n",
    "    print(f\"âœ“ Remaining: {len(df_unique) - len(already_fetched):,} tracks\")\n",
    "    print(f\"âœ“ Progress: {len(already_fetched)/len(df_unique)*100:.1f}%\")\n",
    "    \n",
    "    # Show some stats\n",
    "    print(f\"\\nðŸ“ˆ Dataset Statistics:\")\n",
    "    print(f\"   Multi-artist tracks: {(df_existing['num_artists'] > 1).sum():,} ({(df_existing['num_artists'] > 1).mean()*100:.1f}%)\")\n",
    "    print(f\"   Avg artists per track: {df_existing['num_artists'].mean():.2f}\")\n",
    "    print(f\"   Tracks with genres: {df_existing['artist_genres'].notna().sum():,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ†• STARTING FRESH\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"No existing progress file found.\")\n",
    "    print(f\"Will create new file: {PROGRESS_FILE}\")\n",
    "    print(f\"Total tracks to fetch: {len(df_unique):,}\")\n",
    "    \n",
    "    already_fetched = set()\n",
    "    df_existing = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ce76b",
   "metadata": {},
   "source": [
    "## 5. Multi-Artist Fetching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765bd906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_artist_info_multi_artists(track_ids, sp_client, batch_size=50, delay=0.5, checkpoint_file=None):\n",
    "    \"\"\"\n",
    "    Fetch artist information for multiple tracks, aggregating data for tracks with multiple artists.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    track_ids : list\n",
    "        List of Spotify track IDs\n",
    "    sp_client : spotipy.Spotify\n",
    "        Authenticated Spotify client\n",
    "    batch_size : int\n",
    "        Number of tracks to fetch per API call (max 50)\n",
    "    delay : float\n",
    "        Delay between batches in seconds\n",
    "    checkpoint_file : str\n",
    "        Path to save checkpoint progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with aggregated artist information\n",
    "    \"\"\"\n",
    "    \n",
    "    artist_data = []\n",
    "    total_tracks = len(track_ids)\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    if checkpoint_file and os.path.exists(checkpoint_file):\n",
    "        checkpoint_df = pd.read_csv(checkpoint_file)\n",
    "        already_done = set(checkpoint_df['track_id'].tolist())\n",
    "        artist_data = checkpoint_df.to_dict('records')\n",
    "        print(f\"ðŸ“‚ Loaded checkpoint: {len(already_done)} tracks already processed\")\n",
    "    else:\n",
    "        already_done = set()\n",
    "    \n",
    "    # Filter out already processed tracks\n",
    "    track_ids = [tid for tid in track_ids if tid not in already_done]\n",
    "    \n",
    "    if len(track_ids) == 0:\n",
    "        print(\"âœ“ All tracks already processed!\")\n",
    "        return pd.DataFrame(artist_data)\n",
    "    \n",
    "    print(f\"Fetching artist data for {len(track_ids)} tracks...\")\n",
    "    \n",
    "    try:\n",
    "        # Process in batches\n",
    "        for i in range(0, len(track_ids), batch_size):\n",
    "            batch_ids = track_ids[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Fetch tracks\n",
    "                tracks_response = sp_client.tracks(batch_ids)\n",
    "                \n",
    "                # Process each track\n",
    "                for track in tracks_response['tracks']:\n",
    "                    if track is None:\n",
    "                        continue\n",
    "                    \n",
    "                    track_id = track['id']\n",
    "                    artists = track['artists']\n",
    "                    \n",
    "                    # Get all artist IDs\n",
    "                    artist_ids = [artist['id'] for artist in artists]\n",
    "                    artist_names = [artist['name'] for artist in artists]\n",
    "                    \n",
    "                    # Fetch detailed info for each artist\n",
    "                    artist_details = []\n",
    "                    for artist_id in artist_ids:\n",
    "                        try:\n",
    "                            artist_info = sp_client.artist(artist_id)\n",
    "                            artist_details.append(artist_info)\n",
    "                            time.sleep(0.05)  # Small delay between artist calls\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: Could not fetch artist {artist_id}: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Aggregate artist information\n",
    "                    if artist_details:\n",
    "                        popularities = [a['popularity'] for a in artist_details]\n",
    "                        followers = [a['followers']['total'] for a in artist_details]\n",
    "                        all_genres = []\n",
    "                        for a in artist_details:\n",
    "                            all_genres.extend(a['genres'])\n",
    "                        \n",
    "                        artist_data.append({\n",
    "                            'track_id': track_id,\n",
    "                            'num_artists': len(artist_details),\n",
    "                            'artist_names': '|'.join(artist_names),\n",
    "                            'primary_artist': artist_names[0] if artist_names else None,\n",
    "                            'avg_artist_popularity': np.mean(popularities) if popularities else None,\n",
    "                            'max_artist_popularity': np.max(popularities) if popularities else None,\n",
    "                            'min_artist_popularity': np.min(popularities) if popularities else None,\n",
    "                            'total_artist_followers': np.sum(followers) if followers else None,\n",
    "                            'avg_artist_followers': np.mean(followers) if followers else None,\n",
    "                            'max_artist_followers': np.max(followers) if followers else None,\n",
    "                            'artist_genres': ','.join(list(set(all_genres))) if all_genres else None\n",
    "                        })\n",
    "                \n",
    "                # Progress update\n",
    "                progress = len(artist_data)\n",
    "                if progress % 100 == 0:\n",
    "                    print(f\"Progress: {progress}/{total_tracks} tracks ({progress/total_tracks*100:.1f}%)\")\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    if checkpoint_file:\n",
    "                        pd.DataFrame(artist_data).to_csv(checkpoint_file, index=False)\n",
    "                \n",
    "                # Delay between batches\n",
    "                time.sleep(delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                if 'rate limit' in str(e).lower() or '429' in str(e):\n",
    "                    print(f\"\\nâš ï¸  Rate limit hit at {len(artist_data)} tracks\")\n",
    "                    print(\"Saving progress and stopping...\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Error processing batch: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸  Interrupted by user. Saving progress...\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_result = pd.DataFrame(artist_data)\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    if checkpoint_file and len(df_result) > 0:\n",
    "        df_result.to_csv(checkpoint_file, index=False)\n",
    "        print(f\"ðŸ’¾ Checkpoint saved: {checkpoint_file}\")\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "print(\"âœ“ Fetching function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f1e91",
   "metadata": {},
   "source": [
    "## 6. Fetch Next Batch (RUN THIS DAILY)\n",
    "\n",
    "**âš ï¸ Run this cell once per day to fetch the next batch of tracks.**\n",
    "\n",
    "The function will:\n",
    "- Automatically pick up where you left off\n",
    "- Fetch ~1,500 tracks (or until rate limit)\n",
    "- Save progress to `artist_info_extended.csv`\n",
    "- Show you updated statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c992a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE_PER_DAY = 1500  # Target tracks per day\n",
    "API_BATCH_SIZE = 50        # Tracks per API call\n",
    "DELAY = 0.5                # Delay between API calls (seconds)\n",
    "CHECKPOINT_FILE = 'daily_fetch_checkpoint.csv'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"ðŸš€ DAILY FETCH - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get tracks that haven't been fetched yet\n",
    "remaining_tracks = df_unique[~df_unique['track_id'].isin(already_fetched)]\n",
    "\n",
    "if len(remaining_tracks) == 0:\n",
    "    print(\"\\nðŸŽ‰ ALL TRACKS FETCHED!\")\n",
    "    print(f\"Total tracks in dataset: {len(df_existing):,}\")\n",
    "    print(\"\\nNothing more to fetch. You have the complete dataset!\")\n",
    "else:\n",
    "    # Get next batch\n",
    "    next_batch = remaining_tracks.head(BATCH_SIZE_PER_DAY)\n",
    "    next_batch_ids = next_batch['track_id'].tolist()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Today's Batch:\")\n",
    "    print(f\"   Tracks to fetch: {len(next_batch_ids):,}\")\n",
    "    print(f\"   Already have: {len(already_fetched):,}\")\n",
    "    print(f\"   Remaining after: {len(remaining_tracks) - len(next_batch_ids):,}\")\n",
    "    print(f\"   Estimated time: ~{len(next_batch_ids) * 0.6 / 60:.0f} minutes\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  Starting fetch at {datetime.now().strftime('%H:%M:%S')}...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fetch the batch\n",
    "    new_artist_data = fetch_artist_info_multi_artists(\n",
    "        next_batch_ids,\n",
    "        sp,\n",
    "        batch_size=API_BATCH_SIZE,\n",
    "        delay=DELAY,\n",
    "        checkpoint_file=CHECKPOINT_FILE\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  Finished at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(f\"â±ï¸  Total time: {elapsed/60:.1f} minutes\")\n",
    "    \n",
    "    # Combine with existing data\n",
    "    if df_existing is not None:\n",
    "        df_combined = pd.concat([df_existing, new_artist_data], ignore_index=True)\n",
    "    else:\n",
    "        df_combined = new_artist_data\n",
    "    \n",
    "    # Remove any duplicates (just in case)\n",
    "    df_combined = df_combined.drop_duplicates(subset=['track_id'], keep='last')\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    df_combined.to_csv(PROGRESS_FILE, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ“ DAILY FETCH COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"   Fetched today: {len(new_artist_data):,} tracks\")\n",
    "    print(f\"   Total in dataset: {len(df_combined):,} tracks\")\n",
    "    print(f\"   Remaining: {len(df_unique) - len(df_combined):,} tracks\")\n",
    "    print(f\"   Overall progress: {len(df_combined)/len(df_unique)*100:.1f}%\")\n",
    "    \n",
    "    # Estimate days remaining\n",
    "    if len(new_artist_data) > 0:\n",
    "        days_remaining = np.ceil((len(df_unique) - len(df_combined)) / len(new_artist_data))\n",
    "        print(f\"\\nâ³ Estimated days to complete: {int(days_remaining)} days\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Saved to: {PROGRESS_FILE}\")\n",
    "    \n",
    "    # Show some stats\n",
    "    print(f\"\\nðŸ“ˆ Dataset Statistics:\")\n",
    "    print(f\"   Multi-artist tracks: {(df_combined['num_artists'] > 1).sum():,} ({(df_combined['num_artists'] > 1).mean()*100:.1f}%)\")\n",
    "    print(f\"   Avg artists per track: {df_combined['num_artists'].mean():.2f}\")\n",
    "    print(f\"   Max artists on track: {df_combined['num_artists'].max()}\")\n",
    "    print(f\"   Tracks with genres: {df_combined['artist_genres'].notna().sum():,}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Come back tomorrow to fetch the next batch!\")\n",
    "    print(f\"âœ“ Run this same cell again after 24 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2b5d2",
   "metadata": {},
   "source": [
    "## 7. View Sample of Today's Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c37122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of fetched data\n",
    "if 'new_artist_data' in locals() and len(new_artist_data) > 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"SAMPLE OF TODAY'S FETCHED DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show tracks with multiple artists\n",
    "    multi_artist = new_artist_data[new_artist_data['num_artists'] > 1].head(10)\n",
    "    \n",
    "    if len(multi_artist) > 0:\n",
    "        print(\"\\nTracks with Multiple Artists:\")\n",
    "        display(multi_artist[['artist_names', 'num_artists', 'avg_artist_popularity', \n",
    "                              'total_artist_followers']].head())\n",
    "    \n",
    "    # Show overall sample\n",
    "    print(\"\\nRandom Sample:\")\n",
    "    display(new_artist_data[['primary_artist', 'num_artists', 'avg_artist_popularity', \n",
    "                             'max_artist_popularity', 'artist_genres']].sample(min(10, len(new_artist_data))))\n",
    "else:\n",
    "    print(\"No new data fetched in this session. Run the fetch cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171c1cb",
   "metadata": {},
   "source": [
    "## 8. Cleanup Checkpoint File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the daily checkpoint file (optional - run after successful fetch)\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "    print(f\"âœ“ Cleaned up checkpoint file: {CHECKPOINT_FILE}\")\n",
    "else:\n",
    "    print(\"No checkpoint file to clean up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129fbe5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Instructions for Daily Use:\n",
    "\n",
    "1. **Wait 24 hours** from your last fetch\n",
    "2. **Open this notebook**\n",
    "3. **Run cells 1-6** to load everything\n",
    "4. **Run cell 6** (\"Fetch Next Batch\") to get today's tracks\n",
    "5. **Check progress** in the output\n",
    "6. **Repeat tomorrow!**\n",
    "\n",
    "The notebook will automatically:\n",
    "- Track your progress\n",
    "- Fetch only unfetched tracks\n",
    "- Save everything to `artist_info_extended.csv`\n",
    "- Stop gracefully if rate limit is hit\n",
    "\n",
    "**After ~18-20 days, you'll have all 28K tracks! ðŸŽ‰**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
